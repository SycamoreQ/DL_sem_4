import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
from timm.models.layers import DropPath
from torch_geometric.nn import GCNConv
from timm.models.registry import register_model
import torch 
from torch_geometric.data import Data, Batch
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
from torchvision import transforms
from torch import nn 
from torch import optim
from train_eval import train_model , test_final_model
from torch.utils.data import SubsetRandomSampler
from train_eval import plot_training_history
from vihg import DeepGCN
from vihg import HyperTransformer
from sklearn.model_selection import train_test_split
import os 
from datetime import datetime


class BasicConv(nn.Module):
    """
    Basic convolutional module consisting of a Conv2d, BatchNorm2d and optionally an activation layer.
    """
    def __init__(self, channels, act='relu', norm=True, bias=True):
        super(BasicConv, self).__init__()
        self.channels = channels
        self.act = act
        self.norm = norm
        self.bias = bias
        self.m = nn.Sequential(
            nn.Conv2d(channels[0], channels[1], 1, bias=bias),
            nn.BatchNorm2d(channels[1]) if norm else nn.Identity(),
            self.make_activation(act) if act else nn.Identity()
        )

    def make_activation(self, act):
        return nn.ReLU(inplace=True) if act == 'relu' else nn.Identity()

    def forward(self, x):
        return self.m(x)


def batched_index_select(x, idx):
    """
    Batched index select with protection against invalid indices
    :param x: input tensor (batch_size, num_dims, num_points, 1)
    :param idx: indices tensor (batch_size, num_samples, k)
    :return: indexed tensor (batch_size, num_dims, num_samples, k)
    """
    batch_size, num_dims, num_points = x.shape[:3]
    k = idx.shape[-1]
    
    # Check if idx contains any valid indices
    if (idx < 0).all():
        return torch.zeros(batch_size, num_dims, idx.shape[1], k, device=x.device)
    
    valid_idx = torch.clamp(idx, min=0)
    
    idx_base = torch.arange(0, batch_size, device=idx.device).view(-1, 1, 1) * num_points
    idx = valid_idx + idx_base
    idx = idx.view(-1)

    x = x.transpose(2, 1).contiguous()
    feature = x.view(batch_size * num_points, -1)[idx, :]
    feature = feature.view(batch_size, -1, k, num_dims).permute(0, 3, 1, 2).contiguous()
    
    # Create mask for invalid indices and zero them out
    mask = (valid_idx >= 0).unsqueeze(1).expand(-1, num_dims, -1, -1)
    feature = feature * mask.float()
    
    return feature


def get_2d_relative_pos_embed(embed_dim, grid_size):
    """
    Grid_size: int of the grid height and width
    return:
    pos_embed: [grid_size*grid_size, embed_dim]
    """
    grid_h = np.arange(grid_size, dtype=np.float32)
    grid_w = np.arange(grid_size, dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h) 
    grid = np.stack(grid, axis=0)

    grid = grid.reshape([2, 1, grid_size, grid_size])
    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
    return pos_embed


def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):
    assert embed_dim % 2 == 0

    # use half of dimensions to encode grid_h
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)

    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)
    return emb


def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: output dimension for each position
    pos: a list of positions to be encoded: size (M,)
    out: (M, D)
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=np.float32)
    omega /= embed_dim / 2.
    omega = 1. / 10000**omega  # (D/2,)

    pos = pos.reshape(-1)  # (M,)
    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product

    emb_sin = np.sin(out)  # (M, D/2)
    emb_cos = np.cos(out)  # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
    return emb


def initialize_memberships(batch_size, n_points, n_clusters, device):
    """
    Initialize the membership matrix for Fuzzy C-Means clustering.
    """
    memberships = torch.rand(batch_size, n_points, n_clusters, device=device)
    memberships = memberships / memberships.sum(dim=2, keepdim=True)
    return memberships


def fuzzy_c_means(x, n_clusters, m=2, epsilon=1e-6, max_iter=100):
    """
    Fuzzy C-Means clustering
    """
    batch_size, num_dims, num_points, _ = x.size()
    x = x.squeeze(-1).transpose(1, 2)  # Shape: (batch_size, num_points, num_dims)

    # Initialize the membership matrix
    memberships = initialize_memberships(batch_size, num_points, n_clusters, x.device)

    # Initialize cluster centers
    centers = torch.zeros(batch_size, num_dims, n_clusters, device=x.device)
    prev_memberships = torch.zeros_like(memberships)

    for iteration in range(max_iter):
        for cluster in range(n_clusters):
            weights = memberships[:, :, cluster] ** m
            denominator = weights.sum(dim=1, keepdim=True)
            numerator = (weights.unsqueeze(2) * x).sum(dim=1)
            centers[:, :, cluster] = numerator / denominator

        for cluster in range(n_clusters):
            diff = x - centers[:, :, cluster].unsqueeze(1)
            dist = torch.norm(diff, p=2, dim=2)  # Euclidean distance
            memberships[:, :, cluster] = 1.0 / (dist ** (2 / (m - 1)))

        # Normalize the memberships
        memberships_sum = memberships.sum(dim=2, keepdim=True)
        memberships = memberships / memberships_sum

        # Check convergence
        if iteration > 0 and torch.norm(prev_memberships - memberships) < epsilon:
            break
        prev_memberships = memberships.clone()

    return memberships, centers


def construct_hyperedges(x, num_clusters, threshold=0.5, m=2):
    """
    Constructs hyperedges based on fuzzy c-means clustering.
    Modified to ensure no empty hyperedges.
    """
    with torch.no_grad():
        x = x.detach()
        batch_size, num_dims, num_points, _ = x.shape
        
        memberships, centers = fuzzy_c_means(x, num_clusters, m)
        
        min_threshold = threshold
        while True:
            has_points = (memberships > min_threshold).sum(dim=1) > 0
            if has_points.all():
                break
            min_threshold *= 0.9  # Gradually reduce threshold
            if min_threshold < 0.05:  # Set a lower limit to avoid too low threshold
                for b in range(batch_size):
                    for c in range(num_clusters):
                        if not has_points[b, c]:
                            max_idx = torch.argmax(memberships[b, :, c])
                            memberships[b, max_idx, c] = threshold + 0.01
                break
        
        # Create hyperedge matrix to represent each hyperedge's points
        max_points_per_edge = torch.max((memberships > min_threshold).sum(dim=1)).item()
        max_points_per_edge = max(max_points_per_edge, 1)  
        
        hyperedge_matrix = torch.zeros(batch_size, num_clusters, max_points_per_edge, 
                                     dtype=torch.long, device=x.device)
        
        for b in range(batch_size):
            for c in range(num_clusters):
                idxs = torch.where(memberships[b, :, c] > min_threshold)[0]
                if len(idxs) == 0: 
                    idxs = torch.tensor([torch.argmax(memberships[b, :, c])], device=x.device)
                hyperedge_matrix[b, c, :len(idxs)] = idxs
        
        # Create point to hyperedge index
        max_edges_per_point = torch.max((memberships > min_threshold).sum(dim=-1)).item()
        max_edges_per_point = max(max_edges_per_point, 1) 
        
        point_hyperedge_index = torch.zeros(batch_size, num_points, max_edges_per_point, 
                                          dtype=torch.long, device=x.device)
        
        for b in range(batch_size):
            for p in range(num_points):
                idxs = torch.where(memberships[b, p, :] > min_threshold)[0]
                if len(idxs) == 0: 
                    idxs = torch.tensor([torch.argmax(memberships[b, p, :])], device=x.device)
                point_hyperedge_index[b, p, :len(idxs)] = idxs
    
    return hyperedge_matrix, point_hyperedge_index, centers


class HypergraphConv2d(nn.Module):
    """
    Hypergraph Convolution based on the GIN mechanism
    Modified to handle empty hyperedges
    """
    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):
        super(HypergraphConv2d, self).__init__()
        self.nn_node_to_hyperedge = BasicConv([in_channels, in_channels], act, norm, bias)
        self.nn_hyperedge_to_node = BasicConv([in_channels, out_channels], act, norm, bias)
        eps_init = 0.0
        self.eps = nn.Parameter(torch.Tensor([eps_init]))

    def forward(self, x, hyperedge_matrix, point_hyperedge_index, centers):
        batch_size, num_dims, num_points, _ = x.shape
        
        node_features_for_hyperedges = batched_index_select(x, hyperedge_matrix)
        
        if node_features_for_hyperedges.numel() == 0:
            out = torch.zeros(batch_size, num_dims * 2, num_points, 1, device=x.device)
            return out
            
        aggregated_hyperedge_features = node_features_for_hyperedges.sum(dim=-1, keepdim=True)
        
        reshaped_hyperedge_features = aggregated_hyperedge_features.permute(0, 1, 3, 2)
        aggregated_hyperedge_features = self.nn_node_to_hyperedge(reshaped_hyperedge_features)
        
        aggregated_hyperedge_features = aggregated_hyperedge_features.permute(0, 1, 3, 2)
        centers_expanded = centers.unsqueeze(-1)  # Shape: [B, C, N, 1]
        aggregated_hyperedge_features = aggregated_hyperedge_features + (1 + self.eps) * centers_expanded
        
        # Step 2: Aggregate hyperedge features to update node features
        hyperedge_features_for_nodes = batched_index_select(aggregated_hyperedge_features, point_hyperedge_index)
        
        # Fix: Properly reshape before the second convolution
        reshaped_node_features = hyperedge_features_for_nodes.sum(dim=-1, keepdim=True).permute(0, 1, 3, 2)
        aggregated_node_features_from_hyperedges = self.nn_hyperedge_to_node(reshaped_node_features)
        
        # Reshape back to original format
        out = aggregated_node_features_from_hyperedges.permute(0, 1, 3, 2)

        return out


class HyperGrapher(nn.Module):
    """
    HyperGrapher module exclusively using hypergraph convolution
    """
    def __init__(self, in_channels, num_clusters=50, act='relu', norm=True, bias=True, drop_path=0.0, relative_pos=False, n=196, threshold=0.5, m=2):
        super(HyperGrapher, self).__init__()
        self.in_channels = in_channels
        self.num_clusters = num_clusters
        self.threshold = threshold
        self.m = m
        self.n = n
        
        self.fc1 = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0),
            nn.BatchNorm2d(in_channels),
        )
        
        self.hypergraph_conv = HypergraphConv2d(in_channels, in_channels * 2, act, norm, bias)
        
        self.fc2 = nn.Sequential(
            nn.Conv2d(in_channels * 2, in_channels, 1, stride=1, padding=0),
            nn.BatchNorm2d(in_channels),
        )
        
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        
        # Initialize relative position embedding if needed
        self.relative_pos = None
        if relative_pos:
            relative_pos_tensor = torch.from_numpy(
                np.float32(get_2d_relative_pos_embed(in_channels, int(n**0.5)))
            ).unsqueeze(0).unsqueeze(1)
            self.relative_pos = nn.Parameter(-relative_pos_tensor.squeeze(1), requires_grad=False)

    def forward(self, x):
        _tmp = x
    
        x = self.fc1(x)
        
        B, C, H, W = x.shape
        x_reshaped = x.reshape(B, C , -1 , 1).contiguous()
        
        hyperedge_matrix, point_hyperedge_index, centers = construct_hyperedges(
            x_reshaped, 
            num_clusters=self.num_clusters,
            threshold=self.threshold,
            m=self.m
        )
        
        x_conv = self.hypergraph_conv(x_reshaped, hyperedge_matrix, point_hyperedge_index, centers)
        
        x_conv = x_conv.reshape(B, -1, H, W).contiguous()
        x_conv = self.fc2(x_conv)
        
        x = self.drop_path(x_conv) + _tmp
        
        return x
    

class GPSConv(nn.Module):
    """
    GPS Convolution layer adapted for hypergraph implementation
    """
    def __init__(self, channels, heads=1, conv=None, act='relu', norm='layernorm', attn_type='multihead', dropout=0.0):
        super(GPSConv, self).__init__()
        self.channels = channels
        self.heads = heads
        self.conv = conv
        self.dropout = dropout
        
        if norm == 'layernorm':
            self.norm = nn.LayerNorm(channels)
        else:
            self.norm = nn.Identity()
        
        if act == 'relu':
            self.activation = nn.ReLU()
        elif act == 'gelu':
            self.activation = nn.GELU()
        else:
            self.activation = nn.Identity()
        
        if attn_type == 'multihead':
            self.attn = nn.MultiheadAttention(channels, heads, dropout=dropout)
        else:
            self.attn = nn.Identity()
        
        self.q_linear = nn.Linear(channels, channels)
        self.k_linear = nn.Linear(channels, channels)
        self.v_linear = nn.Linear(channels, channels)
        self.out_linear = nn.Linear(channels, channels)
        
    def forward(self, x):
        # the GPS convolution to work directly with tensor inputs (B, C, H, W)
        B, C, H, W = x.shape
        
        # Reshape to (H*W, B, C) for attention
        x_attn = x.reshape(B, C, H*W).permute(2, 0, 1)
        
        x_norm = self.norm(x_attn)
    
        q = self.q_linear(x_norm)
        k = self.k_linear(x_norm)
        v = self.v_linear(x_norm)
        
        attn_output, _ = self.attn(q, k, v)
        attn_output = self.out_linear(attn_output)
        
        if self.dropout > 0:
            attn_output = F.dropout(attn_output, p=self.dropout, training=self.training)
        
        x_attn = x_attn + attn_output
        
        x_out = x_attn.permute(1, 2, 0).reshape(B, C, H, W)
        
        return x_out


class FFN(nn.Module):
    """
    Feed-Forward Network with residual connection
    """
    def __init__(self, in_features, hidden_features=None, out_features=None, act='relu', drop_path=0.0):
        super(FFN, self).__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        
        self.fc1 = nn.Conv2d(in_features, hidden_features, 1, stride=1, padding=0)
        self.act = nn.ReLU() if act == 'relu' else nn.GELU()
        self.fc2 = nn.Conv2d(hidden_features, out_features, 1, stride=1, padding=0)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(self, x):
        shortcut = x
        x = self.fc1(x)
        x = self.act(x)
        x = self.fc2(x)
        x = self.drop_path(x) + shortcut
        return x


class Seq(nn.Module):
    """
    Sequential module that supports returning the first element
    """
    def __init__(self, *args):
        super(Seq, self).__init__()
        if len(args) == 1 and isinstance(args[0], list):
            args = args[0]
        self.args = nn.ModuleList(args)

    def forward(self, x):
        for module in self.args:
            x = module(x)
        return x


class HyperGraphTransformer(nn.Module):
    """
    Hypergraph Transformer backbone model
    """
    def __init__(self, 
                 num_classes=2, 
                 in_chans=3, 
                 embed_dim=64, 
                 depth=12,
                 hidden_channels=64,
                 num_clusters=32,
                 drop_path_rate=0.1,
                 threshold=0.5,
                 m=1.5,
                 global_pool='avg'):
        super(HyperGraphTransformer, self).__init__()
        self.num_classes = num_classes
        self.num_features = self.embed_dim = embed_dim
        self.n_blocks = depth
        self.global_pool = global_pool
    
        self.stem = nn.Sequential(
            nn.Conv2d(in_chans, embed_dim, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(embed_dim),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )
        
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        
        self.backbone = Seq(*[Seq(
            HyperGrapher(
                in_channels=hidden_channels,
                num_clusters=num_clusters,
                act='relu',
                norm=True,
                bias=True,
                drop_path=dpr[i],
                threshold=threshold,
                m=m
            ),
            FFN(
                in_features=hidden_channels,
                hidden_features=hidden_channels * 4,
                act='relu',
                drop_path=dpr[i]
            ),
            GPSConv(
                channels=hidden_channels,
                heads=4,
                act='gelu',
                norm='layernorm',
                attn_type='multihead',
                dropout=0.1
            )
        ) for i in range(self.n_blocks)])
        
        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
        
        # Initialize weights
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward_features(self, x):
        x = self.stem(x)
        x = self.backbone(x)
        return x

    def forward(self, x):
        x = self.forward_features(x)
        
        if self.global_pool == 'avg':
            x = F.adaptive_avg_pool2d(x, (1, 1))
        else:
            x = F.adaptive_max_pool2d(x, (1, 1))
        
        x = torch.flatten(x, 1)
        x = self.head(x)
        return x


@register_model
def hypergraph_transformer_small(pretrained=False, **kwargs):
    """
    Small Hypergraph Transformer model
    """
    model = HyperGraphTransformer(
        embed_dim=64,
        depth=12,
        hidden_channels=64,
        num_clusters=32,
        drop_path_rate=0.1,
        **kwargs
    )
    return model


@register_model
def hypergraph_transformer_base(pretrained=False, **kwargs):
    """
    Base Hypergraph Transformer model
    """
    model = HyperGraphTransformer(
        embed_dim=96,
        depth=16,
        hidden_channels=96,
        num_clusters=64,
        drop_path_rate=0.2,
        **kwargs
    )
    return model


@register_model
def hypergraph_transformer_large(pretrained=False, **kwargs):
    """
    Large Hypergraph Transformer model
    """
    model = HyperGraphTransformer(
        embed_dim=128,
        depth=24,
        hidden_channels=128,
        num_clusters=80,
        drop_path_rate=0.3,
        **kwargs
    )
    return model
    

torch.manual_seed(42)
np.random.seed(42)

# Get current timestamp for run identification
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
run_dir = f"runs/{timestamp}"
os.makedirs(run_dir, exist_ok=True)
checkpoint_dir = f"{run_dir}/checkpoints"
os.makedirs(checkpoint_dir, exist_ok=True)


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.2),
    transforms.RandomRotation(30),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.RandomGrayscale(p=0.05),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    transforms.RandomErasing(p=0.2)
])

transform_val = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


print("Loading datasets...")
datadir = 'C:/Users/admin/Desktop/DL_SEM_4_part2/DL_sem_4/brain_tumor_dataset'  
train_dataset = ImageFolder(datadir, transform=transform_train)
val_dataset = ImageFolder(datadir, transform=transform_val)

print(f"Dataset classes: {train_dataset.classes}")
print(f"Total samples: {len(train_dataset)}")

indices = list(range(len(train_dataset)))
labels = [train_dataset[i][1] for i in indices]

# Use stratified sampling to ensure class balance
train_indices, temp_indices = train_test_split(
    indices, test_size=0.3, stratify=labels, random_state=42
)
val_indices, test_indices = train_test_split(
    temp_indices, test_size=0.5, stratify=[labels[i] for i in temp_indices], random_state=42
)

print(f"Train samples: {len(train_indices)}")
print(f"Validation samples: {len(val_indices)}")
print(f"Test samples: {len(test_indices)}")

# Calculate class distribution in each split
train_labels = [labels[i] for i in train_indices]
val_labels = [labels[i] for i in val_indices]
test_labels = [labels[i] for i in test_indices]

for split_name, split_labels in [("Train", train_labels), ("Validation", val_labels), ("Test", test_labels)]:
    class_counts = {}
    for label in split_labels:
        if label not in class_counts:
            class_counts[label] = 0
        class_counts[label] += 1
    
    print(f"\n{split_name} class distribution:")
    for class_idx, count in class_counts.items():
        class_name = train_dataset.classes[class_idx]
        percentage = (count / len(split_labels)) * 100
        print(f"  {class_name}: {count} samples ({percentage:.2f}%)")

train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)
test_sampler = SubsetRandomSampler(test_indices)


print("Creating data loaders...")
batch_size = 4
train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler)
test_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=test_sampler)


print("Initializing model...")
model = hypergraph_transformer_small(num_classes=len(train_dataset.classes)).to(device)
print(f"Model initialized with {sum(p.numel() for p in model.parameters())} parameters")


criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.01)  
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6
)


print("-" * 50)
print("Training configuration:")
print(f"Number of epochs: 50")
print(f"Learning rate: {optimizer.param_groups[0]['lr']}")
print(f"Weight decay: {optimizer.param_groups[0]['weight_decay']}")
print(f"Batch size: {batch_size}")
print(f"Run directory: {run_dir}")
print("-" * 50)

try:

    print("Starting training...")
    model, history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        num_epochs=10, 
        device=device 
    )
    

    plot_training_history(history, save_path='training_history.png')
    
    
    print("Evaluating on test set...")
    test_final_model(
        model=model,
        test_loader=test_loader,
        criterion=criterion,
        device=device,
        class_names=train_dataset.classes,
        save_dir=run_dir
    )
    
except Exception as e:
    print(f"An error occurred during training: {str(e)}")
    import traceback
    traceback.print_exc()
    print("Check your model implementation and try again.")


torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'classes': train_dataset.classes
}, f'{run_dir}/final_model.pth')

print(f"Training completed. All results saved to {run_dir}")


